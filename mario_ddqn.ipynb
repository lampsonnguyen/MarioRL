{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "747a7e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from tqdm import tqdm\n",
    "import pickle \n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY, SIMPLE_MOVEMENT, COMPLEX_MOVEMENT\n",
    "import gym\n",
    "import numpy as np\n",
    "import collections \n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import time\n",
    "import pylab as pl\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2d92855",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    \"\"\"\n",
    "        Each action of the agent is repeated over skip frames\n",
    "        return only every `skip`-th frame\n",
    "    \"\"\"\n",
    "    def __init__(self, env=None, skip=4):\n",
    "        super(MaxAndSkipEnv, self).__init__(env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = collections.deque(maxlen=2)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            self._obs_buffer.append(obs)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Clear past frame buffer and init to first obs\"\"\"\n",
    "        self._obs_buffer.clear()\n",
    "        obs = self.env.reset()\n",
    "        self._obs_buffer.append(obs)\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aa1341a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarioRescale84x84(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Downsamples/Rescales each frame to size 84x84 with greyscale\n",
    "    \"\"\"\n",
    "    def __init__(self, env=None):\n",
    "        super(MarioRescale84x84, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return MarioRescale84x84.process(obs)\n",
    "\n",
    "    @staticmethod\n",
    "    def process(frame):\n",
    "        if frame.size == 240 * 256 * 3:\n",
    "            img = np.reshape(frame, [240, 256, 3]).astype(np.float32)\n",
    "        else:\n",
    "            assert False, \"Unknown resolution.\" \n",
    "        # image normalization on RBG\n",
    "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
    "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
    "        x_t = resized_screen[18:102, :]\n",
    "        x_t = np.reshape(x_t, [84, 84, 1])\n",
    "        return x_t.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "915d4c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Each frame is converted to PyTorch tensors\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.moveaxis(observation, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49ea3fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BufferWrapper(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Only every k-th frame is collected by the buffer\n",
    "    \"\"\"\n",
    "    def __init__(self, env, n_steps, dtype=np.float32):\n",
    "        super(BufferWrapper, self).__init__(env)\n",
    "        self.dtype = dtype\n",
    "        old_space = env.observation_space\n",
    "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
    "                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
    "\n",
    "    def reset(self):\n",
    "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
    "        return self.observation(self.env.reset())\n",
    "\n",
    "    def observation(self, observation):\n",
    "        self.buffer[:-1] = self.buffer[1:]\n",
    "        self.buffer[-1] = observation\n",
    "        return self.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc293b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelNormalization(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Normalize pixel values in frame --> 0 to 1\n",
    "    \"\"\"\n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7c119a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mario_env(env):\n",
    "    env = MaxAndSkipEnv(env)\n",
    "    env = MarioRescale84x84(env)\n",
    "    env = ImageToPyTorch(env)\n",
    "    env = BufferWrapper(env, 4)\n",
    "    env = PixelNormalization(env)\n",
    "    return JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb35e859",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNSolver(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Neural Net with 3 conv layers and two linear layers\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQNSolver, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "    \n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37cc217d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "\n",
    "    def __init__(self, state_space, action_space, max_memory_size, batch_size, gamma, lr,\n",
    "                 dropout, exploration_max, exploration_min, exploration_decay, double_dqn, pretrained):\n",
    "\n",
    "        # Define DQN Layers\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.double_dqn = double_dqn\n",
    "        self.pretrained = pretrained\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        # Double DQN network\n",
    "        if self.double_dqn:  \n",
    "            self.local_net = DQNSolver(state_space, action_space).to(self.device)\n",
    "            self.target_net = DQNSolver(state_space, action_space).to(self.device)\n",
    "            \n",
    "            if self.pretrained:\n",
    "                self.local_net.load_state_dict(torch.load(\"DQN1.pt\", map_location=torch.device(self.device)))\n",
    "                self.target_net.load_state_dict(torch.load(\"DQN2.pt\", map_location=torch.device(self.device)))\n",
    "                    \n",
    "            self.optimizer = torch.optim.Adam(self.local_net.parameters(), lr=lr)\n",
    "            self.copy = 5000  # Copy the local model weights into the target network every 5000 steps\n",
    "            self.step = 0\n",
    "        # DQN network\n",
    "        else:  \n",
    "            self.dqn = DQNSolver(state_space, action_space).to(self.device)\n",
    "            \n",
    "            if self.pretrained:\n",
    "                self.dqn.load_state_dict(torch.load(\"DQN.pt\", map_location=torch.device(self.device)))\n",
    "            self.optimizer = torch.optim.Adam(self.dqn.parameters(), lr=lr)\n",
    "\n",
    "        # Create memory\n",
    "        self.max_memory_size = max_memory_size\n",
    "        if self.pretrained:\n",
    "            self.STATE_MEM = torch.load(\"STATE_MEM.pt\")\n",
    "            self.ACTION_MEM = torch.load(\"ACTION_MEM.pt\")\n",
    "            self.REWARD_MEM = torch.load(\"REWARD_MEM.pt\")\n",
    "            self.STATE2_MEM = torch.load(\"STATE2_MEM.pt\")\n",
    "            self.DONE_MEM = torch.load(\"DONE_MEM.pt\")\n",
    "            with open(\"ending_position.pkl\", 'rb') as f:\n",
    "                self.ending_position = pickle.load(f)\n",
    "            with open(\"num_in_queue.pkl\", 'rb') as f:\n",
    "                self.num_in_queue = pickle.load(f)\n",
    "        else:\n",
    "            self.STATE_MEM = torch.zeros(max_memory_size, *self.state_space)\n",
    "            self.ACTION_MEM = torch.zeros(max_memory_size, 1)\n",
    "            self.REWARD_MEM = torch.zeros(max_memory_size, 1)\n",
    "            self.STATE2_MEM = torch.zeros(max_memory_size, *self.state_space)\n",
    "            self.DONE_MEM = torch.zeros(max_memory_size, 1)\n",
    "            self.ending_position = 0\n",
    "            self.num_in_queue = 0\n",
    "        \n",
    "        self.memory_sample_size = batch_size\n",
    "        \n",
    "        # Learning parameters\n",
    "        self.gamma = gamma\n",
    "        self.l1 = nn.SmoothL1Loss().to(self.device) # Also known as Huber loss\n",
    "        self.exploration_max = exploration_max\n",
    "        self.exploration_rate = exploration_max\n",
    "        self.exploration_min = exploration_min\n",
    "        self.exploration_decay = exploration_decay\n",
    "\n",
    "    def remember(self, state, action, reward, state2, done):\n",
    "        \"\"\"Store the experiences in a buffer to use later\"\"\"\n",
    "        self.STATE_MEM[self.ending_position] = state.float()\n",
    "        self.ACTION_MEM[self.ending_position] = action.float()\n",
    "        self.REWARD_MEM[self.ending_position] = reward.float()\n",
    "        self.STATE2_MEM[self.ending_position] = state2.float()\n",
    "        self.DONE_MEM[self.ending_position] = done.float()\n",
    "        self.ending_position = (self.ending_position + 1) % self.max_memory_size  # FIFO tensor\n",
    "        self.num_in_queue = min(self.num_in_queue + 1, self.max_memory_size)\n",
    "    \n",
    "    def batch_experiences(self):\n",
    "        \"\"\"Randomly sample 'batch size' experiences\"\"\"\n",
    "        idx = random.choices(range(self.num_in_queue), k=self.memory_sample_size)\n",
    "        STATE = self.STATE_MEM[idx]\n",
    "        ACTION = self.ACTION_MEM[idx]\n",
    "        REWARD = self.REWARD_MEM[idx]\n",
    "        STATE2 = self.STATE2_MEM[idx]\n",
    "        DONE = self.DONE_MEM[idx]      \n",
    "        return STATE, ACTION, REWARD, STATE2, DONE\n",
    "    \n",
    "    def act(self, state):\n",
    "        \"\"\"Epsilon-greedy action\"\"\"\n",
    "        if self.double_dqn:\n",
    "            self.step += 1\n",
    "        if random.random() < self.exploration_rate:  \n",
    "            return torch.tensor([[random.randrange(self.action_space)]])\n",
    "        if self.double_dqn:\n",
    "            # Local net is used for the policy\n",
    "            return torch.argmax(self.local_net(state.to(self.device))).unsqueeze(0).unsqueeze(0).cpu()\n",
    "        else:\n",
    "            return torch.argmax(self.dqn(state.to(self.device))).unsqueeze(0).unsqueeze(0).cpu()\n",
    "    \n",
    "    def copy_model(self):\n",
    "        \"\"\"Copy local net weights into target net for DDQN network\"\"\"\n",
    "        self.target_net.load_state_dict(self.local_net.state_dict())\n",
    "    \n",
    "    def experience_replay(self):\n",
    "        \"\"\"Use the double Q-update or Q-update equations to update the network weights\"\"\"\n",
    "        if self.double_dqn and self.step % self.copy == 0:\n",
    "            self.copy_model()\n",
    "\n",
    "        if self.memory_sample_size > self.num_in_queue:\n",
    "            return\n",
    "    \n",
    "        # Sample a batch of experiences\n",
    "        STATE, ACTION, REWARD, STATE2, DONE = self.batch_experiences()\n",
    "        STATE = STATE.to(self.device)\n",
    "        ACTION = ACTION.to(self.device)\n",
    "        REWARD = REWARD.to(self.device)\n",
    "        STATE2 = STATE2.to(self.device)\n",
    "        DONE = DONE.to(self.device)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        if self.double_dqn:\n",
    "            # Double Q-Learning target is Q*(S, A) <- r + γ max_a Q_target(S', a)\n",
    "            target = REWARD + torch.mul((self.gamma * self.target_net(STATE2).max(1).values.unsqueeze(1)),  1 - DONE)\n",
    "\n",
    "            current = self.local_net(STATE).gather(1, ACTION.long()) # Local net approximation of Q-value\n",
    "        else:\n",
    "            # Q-Learning target is Q*(S, A) <- r + γ max_a Q(S', a) \n",
    "            target = REWARD + torch.mul((self.gamma * self.dqn(STATE2).max(1).values.unsqueeze(1)), 1 - DONE)\n",
    "                \n",
    "            current = self.dqn(STATE).gather(1, ACTION.long())\n",
    "        \n",
    "        loss = self.l1(current, target)\n",
    "        loss.backward() # Compute gradients\n",
    "        self.optimizer.step() # Backpropagate error\n",
    "\n",
    "        self.exploration_rate *= self.exploration_decay\n",
    "        \n",
    "        # Makes sure that exploration rate is always at least 'exploration min'\n",
    "        self.exploration_rate = max(self.exploration_rate, self.exploration_min)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b93ba5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_state(env, ep=0, info=\"\"):\n",
    "    \"\"\"While testing show the mario playing environment\"\"\"\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"Episode: %d %s\" % (ep, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c1e914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(training_mode, pretrained, double_dqn, world, level, num_episodes=1000, exploration_max=1):\n",
    "    game_level = 'SuperMarioBros-' + str(world) + '-' + str(level) + '-v0'\n",
    "    env = gym_super_mario_bros.make(game_level)\n",
    "    env = create_mario_env(env)  # Wraps the environment so that frames are grayscale \n",
    "    observation_space = env.observation_space.shape\n",
    "    action_space = env.action_space.n\n",
    "    agent = DQNAgent(state_space=observation_space,\n",
    "                     action_space=action_space,\n",
    "                     max_memory_size=30000,\n",
    "                     batch_size=32,\n",
    "                     gamma=0.90,\n",
    "                     lr=0.00025,\n",
    "                     dropout=0.2,\n",
    "                     exploration_max=1.0,\n",
    "                     exploration_min=0.02,\n",
    "                     exploration_decay=0.99,\n",
    "                     double_dqn=double_dqn,\n",
    "                     pretrained=pretrained)\n",
    "    \n",
    "    # Restart the enviroment for each episode\n",
    "    num_episodes = num_episodes\n",
    "    env.reset()\n",
    "    \n",
    "    total_rewards = []\n",
    "    if training_mode and pretrained:\n",
    "        with open(\"total_rewards.pkl\", 'rb') as f:\n",
    "            total_rewards = pickle.load(f)\n",
    "    \n",
    "    for ep_num in tqdm(range(num_episodes)):\n",
    "        state = env.reset()\n",
    "        state = torch.Tensor([state])\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        while True:\n",
    "            if not training_mode:\n",
    "                show_state(env, ep_num)\n",
    "            action = agent.act(state)\n",
    "            steps += 1\n",
    "            \n",
    "            state_next, reward, terminal, info = env.step(int(action[0]))\n",
    "            total_reward += reward\n",
    "            state_next = torch.Tensor([state_next])\n",
    "            reward = torch.tensor([reward]).unsqueeze(0)\n",
    "            \n",
    "            terminal = torch.tensor([int(terminal)]).unsqueeze(0)\n",
    "            \n",
    "            if training_mode:\n",
    "                agent.remember(state, action, reward, state_next, terminal)\n",
    "                agent.experience_replay()\n",
    "            \n",
    "            state = state_next\n",
    "            if terminal:\n",
    "                break\n",
    "        \n",
    "        total_rewards.append(total_reward)\n",
    "        \n",
    "        if ep_num != 0 and ep_num % 100 == 0:\n",
    "            print(\"Episode {} score = {}, average score = {}\".format(ep_num + 1, total_rewards[-1], np.mean(total_rewards)))\n",
    "        num_episodes += 1  \n",
    "\n",
    "    print(\"Episode {} score = {}, average score = {}\".format(ep_num + 1, total_rewards[-1], np.mean(total_rewards)))\n",
    "    \n",
    "    # Save the trained memory so that we can continue from where we stop using 'pretrained' = True\n",
    "    if training_mode:\n",
    "        with open(\"ending_position.pkl\", \"wb\") as f:\n",
    "            pickle.dump(agent.ending_position, f)\n",
    "        with open(\"num_in_queue.pkl\", \"wb\") as f:\n",
    "            pickle.dump(agent.num_in_queue, f)\n",
    "        with open(\"total_rewards.pkl\", \"wb\") as f:\n",
    "            pickle.dump(total_rewards, f)\n",
    "        if agent.double_dqn:\n",
    "            torch.save(agent.local_net.state_dict(), \"DQN1.pt\")\n",
    "            torch.save(agent.target_net.state_dict(), \"DQN2.pt\")\n",
    "        else:\n",
    "            torch.save(agent.dqn.state_dict(), \"DQN.pt\")  \n",
    "        torch.save(agent.STATE_MEM,  \"STATE_MEM.pt\")\n",
    "        torch.save(agent.ACTION_MEM, \"ACTION_MEM.pt\")\n",
    "        torch.save(agent.REWARD_MEM, \"REWARD_MEM.pt\")\n",
    "        torch.save(agent.STATE2_MEM, \"STATE2_MEM.pt\")\n",
    "        torch.save(agent.DONE_MEM,   \"DONE_MEM.pt\")\n",
    "    \n",
    "    env.close()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2f2a51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training\n",
    "def train(world, level, num_ep):\n",
    "    run(training_mode=True, pretrained=False, double_dqn=True, world=world, level=level,num_episodes=num_ep, exploration_max = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c99fbfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Testing\n",
    "def test(world, level, ep_num):\n",
    "    run(training_mode=False, pretrained=True, double_dqn=True, world=world, level=level, num_episodes=ep_num, exploration_max = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "684b89ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_world = 1\n",
    "train_level = 3\n",
    "training_ep = 50000\n",
    "\n",
    "#train(train_world, train_level, training_ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f13caea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "world = 1\n",
    "level = 1\n",
    "num_of_tries = 21\n",
    "test(world, level, num_of_tries)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
